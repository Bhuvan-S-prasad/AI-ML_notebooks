{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13286441,"sourceType":"datasetVersion","datasetId":8420428},{"sourceId":13287120,"sourceType":"datasetVersion","datasetId":8420939}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers datasets accelerate peft bitsandbytes trl\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-07T05:38:07.508480Z","iopub.execute_input":"2025-10-07T05:38:07.508675Z","iopub.status.idle":"2025-10-07T05:39:44.224958Z","shell.execute_reply.started":"2025-10-07T05:38:07.508660Z","shell.execute_reply":"2025-10-07T05:39:44.224236Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.6/564.6 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM, \n    TrainingArguments, \n    Trainer,\n    DataCollatorForLanguageModeling\n)\nfrom datasets import load_dataset\nfrom peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\nimport gc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T05:41:38.278560Z","iopub.execute_input":"2025-10-07T05:41:38.279401Z","iopub.status.idle":"2025-10-07T05:42:01.831633Z","shell.execute_reply.started":"2025-10-07T05:41:38.279365Z","shell.execute_reply":"2025-10-07T05:42:01.830889Z"}},"outputs":[{"name":"stderr","text":"2025-10-07 05:41:47.423390: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759815707.620749      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759815707.675338      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"print(f\"GPU Available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T05:42:01.832652Z","iopub.execute_input":"2025-10-07T05:42:01.833175Z","iopub.status.idle":"2025-10-07T05:42:01.838476Z","shell.execute_reply.started":"2025-10-07T05:42:01.833156Z","shell.execute_reply":"2025-10-07T05:42:01.837454Z"}},"outputs":[{"name":"stdout","text":"GPU Available: True\nGPU Name: Tesla T4\nGPU Memory: 15.83 GB\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n\nraw_data = load_dataset(\"json\", data_files=\"/kaggle/input/aug-dataset/Augmented-dataset.json\")\nprint(f\"Dataset loaded: {raw_data}\")\nprint(f\"Sample: {raw_data['train'][0]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T07:00:49.082976Z","iopub.execute_input":"2025-10-07T07:00:49.083252Z","iopub.status.idle":"2025-10-07T07:00:49.821244Z","shell.execute_reply.started":"2025-10-07T07:00:49.083233Z","shell.execute_reply":"2025-10-07T07:00:49.820526Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49c2c1c3a8a94f64a654a6efce107f7f"}},"metadata":{}},{"name":"stdout","text":"Dataset loaded: DatasetDict({\n    train: Dataset({\n        features: ['prompt', 'completion'],\n        num_rows: 197\n    })\n})\nSample: {'prompt': 'What is your Name?', 'completion': 'My name is Bhuvan S.'}\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    model_name,\n    trust_remote_code=True\n)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T07:00:51.033038Z","iopub.execute_input":"2025-10-07T07:00:51.033305Z","iopub.status.idle":"2025-10-07T07:00:51.997723Z","shell.execute_reply.started":"2025-10-07T07:00:51.033278Z","shell.execute_reply":"2025-10-07T07:00:51.996906Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"def preprocess(examples):\n    texts = []\n    for prompt, completion in zip(examples[\"prompt\"], examples[\"completion\"]):\n        messages = [\n            {\"role\": \"user\", \"content\": prompt},\n            {\"role\": \"assistant\", \"content\": completion}\n        ]\n        # Apply chat template\n        text = tokenizer.apply_chat_template(\n            messages, \n            tokenize=False, \n            add_generation_prompt=False\n        )\n        texts.append(text)\n    \n    tokenized = tokenizer(\n        texts,\n        max_length=256,\n        truncation=True,\n        padding=\"max_length\",\n        return_tensors=None\n    )\n    \n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    \n    return tokenized","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T07:00:53.718194Z","iopub.execute_input":"2025-10-07T07:00:53.718678Z","iopub.status.idle":"2025-10-07T07:00:53.723808Z","shell.execute_reply.started":"2025-10-07T07:00:53.718654Z","shell.execute_reply":"2025-10-07T07:00:53.722929Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"tokenized_data = raw_data.map(\n    preprocess, \n    batched=True,\n    remove_columns=raw_data[\"train\"].column_names,\n    desc=\"Tokenizing dataset\"\n)\n\nprint(f\"Tokenized dataset: {tokenized_data}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T07:00:55.089209Z","iopub.execute_input":"2025-10-07T07:00:55.089497Z","iopub.status.idle":"2025-10-07T07:00:55.302786Z","shell.execute_reply.started":"2025-10-07T07:00:55.089462Z","shell.execute_reply":"2025-10-07T07:00:55.302035Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Tokenizing dataset:   0%|          | 0/197 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58edab6444834305874f28b434e5cade"}},"metadata":{}},{"name":"stdout","text":"Tokenized dataset: DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 197\n    })\n})\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",  \n    torch_dtype=torch.float16,\n    trust_remote_code=True,\n    low_cpu_mem_usage=True  \n)\n\nmodel.gradient_checkpointing_enable()\n\nlora_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    r=16,  \n    lora_alpha=32,  \n    lora_dropout=0.05,  \n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  \n    bias=\"none\"\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T07:00:56.603068Z","iopub.execute_input":"2025-10-07T07:00:56.603739Z","iopub.status.idle":"2025-10-07T07:01:05.078903Z","shell.execute_reply.started":"2025-10-07T07:00:56.603703Z","shell.execute_reply":"2025-10-07T07:01:05.078111Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbce0b33d8e24d80a3a280713a070684"}},"metadata":{}},{"name":"stdout","text":"trainable params: 7,372,800 || all params: 3,093,311,488 || trainable%: 0.2383\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"output_dir = \"/kaggle/working/qwen-finetuned\"\n\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=50,  \n    per_device_train_batch_size=2, \n    gradient_accumulation_steps=4, \n    learning_rate=2e-4, \n    lr_scheduler_type=\"cosine\",  \n    warmup_steps=10,  \n    logging_steps=100,  \n    save_strategy=\"epoch\",  \n    save_total_limit=2,  \n    fp16=True,  \n    gradient_checkpointing=True,\n    optim=\"adamw_torch\",  \n    report_to=\"none\", \n    push_to_hub=False,\n    max_grad_norm=1.0, \n    remove_unused_columns=False,  \n    eval_strategy=\"no\", \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T07:02:43.153817Z","iopub.execute_input":"2025-10-07T07:02:43.154364Z","iopub.status.idle":"2025-10-07T07:02:43.186149Z","shell.execute_reply.started":"2025-10-07T07:02:43.154342Z","shell.execute_reply":"2025-10-07T07:02:43.185637Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"data_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False  \n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_data[\"train\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer\n)\n\nprint(\"Starting training...\")\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T07:02:44.697508Z","iopub.execute_input":"2025-10-07T07:02:44.698191Z","iopub.status.idle":"2025-10-07T07:51:39.776110Z","shell.execute_reply.started":"2025-10-07T07:02:44.698166Z","shell.execute_reply":"2025-10-07T07:51:39.775319Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_37/549914613.py:6: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1250/1250 48:51, Epoch 50/50]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.786900</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.268300</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.128900</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.102700</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.088100</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.081700</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.077200</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.075300</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.073800</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.072400</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.071500</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.070900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1250, training_loss=0.15464356327056886, metrics={'train_runtime': 2933.9598, 'train_samples_per_second': 3.357, 'train_steps_per_second': 0.426, 'total_flos': 4.2092764594176e+16, 'train_loss': 0.15464356327056886, 'epoch': 50.0})"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"model.save_pretrained(f\"{output_dir}/final_model_new\")\ntokenizer.save_pretrained(f\"{output_dir}/final_model_new\")\n\nprint(f\"Model saved to {output_dir}/final_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T07:52:16.859521Z","iopub.execute_input":"2025-10-07T07:52:16.859787Z","iopub.status.idle":"2025-10-07T07:52:17.554846Z","shell.execute_reply.started":"2025-10-07T07:52:16.859769Z","shell.execute_reply":"2025-10-07T07:52:17.553973Z"}},"outputs":[{"name":"stdout","text":"Model saved to /kaggle/working/qwen-finetuned/final_model\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"from peft import PeftModel\n\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"cuda\",\n    torch_dtype=torch.float16,\n    trust_remote_code=True\n)\n\nfinetuned_model = PeftModel.from_pretrained(\n    base_model,\n    f\"{output_dir}/final_model_new\"\n)\nfinetuned_model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:18:42.042071Z","iopub.execute_input":"2025-10-07T08:18:42.042636Z","iopub.status.idle":"2025-10-07T08:18:48.942102Z","shell.execute_reply.started":"2025-10-07T08:18:42.042614Z","shell.execute_reply":"2025-10-07T08:18:48.941533Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9575b0db8c1467cb6031c6750ab9fd0"}},"metadata":{}},{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Qwen2ForCausalLM(\n      (model): Qwen2Model(\n        (embed_tokens): Embedding(151936, 2048)\n        (layers): ModuleList(\n          (0-35): 36 x Qwen2DecoderLayer(\n            (self_attn): Qwen2Attention(\n              (q_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=2048, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=256, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=256, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=256, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=256, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n            )\n            (mlp): Qwen2MLP(\n              (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n              (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n              (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n              (act_fn): SiLUActivation()\n            )\n            (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n            (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n          )\n        )\n        (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n        (rotary_emb): Qwen2RotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":57},{"cell_type":"code","source":"test_prompts = [\n    \"What is your name?\",\n    \"Tell me about yourself\",\n    \"What are your technical skills?\",\n    \"what AI techniques are you familiar with and where have you used them?\",\n    \"Do you have any experience that isn't purely technical?\",\n    \"What are you interested in?\",\n    \"What's your specialty?\",\n    \"How can I contact you?\",\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:21:49.204314Z","iopub.execute_input":"2025-10-07T08:21:49.205137Z","iopub.status.idle":"2025-10-07T08:21:49.209039Z","shell.execute_reply.started":"2025-10-07T08:21:49.205104Z","shell.execute_reply":"2025-10-07T08:21:49.208286Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"print(\"=\"*60)\nprint(\"TESTING FINE-TUNED MODEL\")\nprint(\"=\"*60)\n\nfor test_prompt in test_prompts:\n    messages = [{\"role\": \"user\", \"content\": test_prompt}]\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    \n    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n    outputs = finetuned_model.generate(\n        **inputs,\n        max_new_tokens=100,\n        temperature=0.1,\n        do_sample=True,\n        top_p=0.9,\n        repetition_penalty=1.2,\n        pad_token_id=tokenizer.pad_token_id,\n        eos_token_id=tokenizer.eos_token_id\n    )\n    \n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    response = response.split(\"assistant\\n\")[-1] if \"assistant\" in response else response\n    \n    print(f\"\\nprompt: {test_prompt}\")\n    print(f\"Response: {response.strip()}\")\n    print(\"-\"*130)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:21:51.722997Z","iopub.execute_input":"2025-10-07T08:21:51.723591Z","iopub.status.idle":"2025-10-07T08:22:13.566930Z","shell.execute_reply.started":"2025-10-07T08:21:51.723568Z","shell.execute_reply":"2025-10-07T08:22:13.566262Z"}},"outputs":[{"name":"stdout","text":"============================================================\nTESTING FINE-TUNED MODEL\n============================================================\n\nprompt: What is your name?\nResponse: My name is Bhuvan S.\n----------------------------------------------------------------------------------------------------------------------------------\n\nprompt: Tell me about yourself\nResponse: I'm Bhuvan S., an aspiring AI engineer in my final year of college. I love working on end-to-end projects, from data analysis and model training to building APIs and deploying them. My main interests are in deep learning, computer vision, and building practical, explainable AI systems.\n----------------------------------------------------------------------------------------------------------------------------------\n\nprompt: What are your technical skills?\nResponse: I have hands-on experience with several tools, including Git for version control, Python and its libraries for programming, GitHub Actions for automating workflows, Flask for web development, BERT and other NLP models for building intelligent systems, GPUs for accelerated computing, and YAML for configuration files.\n----------------------------------------------------------------------------------------------------------------------------------\n\nprompt: what AI techniques are you familiar with and where have you used them?\nResponse: I am familiar with and have used several advanced techniques in my projects, including Ensembles for combining model predictions, XGBoost for its high performance, and Grad-CAM for explaining model decisions.\n----------------------------------------------------------------------------------------------------------------------------------\n\nprompt: Do you have any experience that isn't purely technical?\nResponse: Yes, I do. For instance, I took on the role of editor for our department's magazine, 'Manaskirthi'. It was a great experience that involved strong communication, organization, and writing skills.\n----------------------------------------------------------------------------------------------------------------------------------\n\nprompt: What are you interested in?\nResponse: I am immensely interested in Artificial Intelligence and its applications. Specifically, I have a keen interest in Deep Learning, Computer Vision, and creating intelligent agents.\n----------------------------------------------------------------------------------------------------------------------------------\n\nprompt: What's your specialty?\nResponse: My main area of specialization is Artificial Intelligence. Specifically, I have a keen interest in Deep Learning and its applications.\n----------------------------------------------------------------------------------------------------------------------------------\n\nprompt: How can I contact you?\nResponse: The best way to contact me is through email at bhuvansbhuvans113@gmail.com.\n----------------------------------------------------------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}