{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMB0T8K2F2hc4iuquasr5WX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Introduction to LangChain\n","\n","LangChain is a powerful framework designed to help developers build applications powered by Large Language Models (LLMs). While LLMs like GPT can generate amazing results, LangChain takes it a step further by allowing these models to:\n","\n","- Connect with **external tools** (like APIs or search engines)\n","- Maintain **memory** across conversations\n","- **Retrieve** documents for context-aware answers\n","- Act like **autonomous agents** that can make decisions\n","\n","In simple terms, LangChain helps you go beyond just sending prompts and receiving responses. It lets you build smart, interactive systems where LLMs can take actions, remember context, and even query databases or documents!\n","\n","This notebook will guide you through the basics of LangChain‚Äîfrom simple prompts to building smart agents‚Äîall using Python and easy-to-understand examples.\n"],"metadata":{"id":"xUCuu5iSWLzY"}},{"cell_type":"markdown","source":["## Langchain modules\n","\n","- Models: Choosing from different LLMs and embedding models\n","- Prompts: Managing LLM inputs\n","- Chains: Combining LLMs with other components\n","- Indexes: Accessing external data\n","- Memory: Remembering previous conversations\n","- Agents: Accessing other tools"],"metadata":{"id":"c15k1oh2q3QZ"}},{"cell_type":"markdown","source":["## Installing the dependancies"],"metadata":{"id":"_VaSM8L3eSxo"}},{"cell_type":"code","source":["# Install LangChain\n","!pip install langchain --quiet\n"],"metadata":{"id":"Qu4P1iduWjmF","executionInfo":{"status":"ok","timestamp":1745660445095,"user_tz":-330,"elapsed":9919,"user":{"displayName":"4MH22CA009","userId":"08774515799825101415"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["!pip install google-ai-generativelanguage --quiet\n"],"metadata":{"id":"v2rJKFqRWnM9","executionInfo":{"status":"ok","timestamp":1745660453892,"user_tz":-330,"elapsed":8793,"user":{"displayName":"4MH22CA009","userId":"08774515799825101415"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!pip install langchain_google_genai --quiet"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2-MgmsrR7D_C","executionInfo":{"status":"ok","timestamp":1745660465300,"user_tz":-330,"elapsed":11395,"user":{"displayName":"4MH22CA009","userId":"08774515799825101415"}},"outputId":"294d3b9b-2075-4e64-bccb-fc4381d5afb4"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.17 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"markdown","source":["### setting up the API key for gemini"],"metadata":{"id":"RSeq03z4eYAI"}},{"cell_type":"code","source":["import os\n","\n","os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCUHXA5HSol4Em-cZvB5E_L2a5nx3dElHI\""],"metadata":{"id":"eA-EYOp-btEX","executionInfo":{"status":"ok","timestamp":1745660465316,"user_tz":-330,"elapsed":14,"user":{"displayName":"4MH22CA009","userId":"08774515799825101415"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["###  Importing Required Libraries\n","\n","\n","- `GoogleGenerativeAI`: the LangChain LLM wrapper for Gemini\n","- `PromptTemplate`: to create reusable prompt formats\n","- `LLMChain`: to link prompts and LLMs into a workflow\n"],"metadata":{"id":"Gg4GLOfxXzD6"}},{"cell_type":"code","source":["from langchain_google_genai import ChatGoogleGenerativeAI\n","from langchain.prompts import PromptTemplate\n","from langchain.chains import LLMChain\n"],"metadata":{"id":"PSLgBRiMXYn_","executionInfo":{"status":"ok","timestamp":1745660470490,"user_tz":-330,"elapsed":5172,"user":{"displayName":"4MH22CA009","userId":"08774515799825101415"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## **Model**: Using Gemini-1.5-flash\n","\n","\n","In LangChain, we can easily specify `model=\"models/gemini-1.5-flash\"` when initializing the `ChatGoogleGenerativeAI` class to use this variant.\n","\n","the `temperature` decides the randomness and creativity of the output from LLM"],"metadata":{"id":"TYPYLhCTcUUH"}},{"cell_type":"code","source":["# Gemini model\n","gemini_flash = ChatGoogleGenerativeAI(\n","    model=\"models/gemini-1.5-flash\",\n","    temperature=0.2\n",")\n"],"metadata":{"id":"Un6xDqIUYaay","executionInfo":{"status":"ok","timestamp":1745660470669,"user_tz":-330,"elapsed":175,"user":{"displayName":"4MH22CA009","userId":"08774515799825101415"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["gemini_flash.invoke(\"who was bit by a radioactive spider?\").content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"EMT8frKOaRGG","executionInfo":{"status":"ok","timestamp":1745660471211,"user_tz":-330,"elapsed":530,"user":{"displayName":"4MH22CA009","userId":"08774515799825101415"}},"outputId":"d168e40c-53c3-46d7-b787-97cefb7fff0e"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Peter Parker, who later became Spider-Man, was bitten by a radioactive spider.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["topic = \"Thor\"\n","\n","gemini_flash.invoke(f\"Give me a fun fact about {topic}.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VvgPBYopaYzq","executionInfo":{"status":"ok","timestamp":1745660669684,"user_tz":-330,"elapsed":605,"user":{"displayName":"4MH22CA009","userId":"08774515799825101415"}},"outputId":"0881b67b-f9fe-448f-f208-ff0eed8d3091"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='Thor\\'s hammer, Mjolnir, is so heavy that only those deemed \"worthy\" can lift it.  But in the comics, there have been several instances where other characters have lifted it ‚Äì including a frog!  (Yes, really!)', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run-4addda6a-f22f-457e-8b08-afc69787d13b-0', usage_metadata={'input_tokens': 8, 'output_tokens': 52, 'total_tokens': 60, 'input_token_details': {'cache_read': 0}})"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["###  Let the User Enter Their Own Topic\n","\n","So far, we‚Äôve used a fixed topic stored in a variable. Now, let‚Äôs make it interactive!\n","\n","We‚Äôll use Python‚Äôs `input()` function to allow the user to type in any topic they‚Äôre curious about. Gemini will then generate a fun fact based on their input.\n"],"metadata":{"id":"eTzBv4BSgHlN"}},{"cell_type":"code","source":["input_prompt = input(\"enter the prompt:\")\n","\n","gemini_flash.invoke(f\"{input_prompt}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"BkKh9SxzdzQY","executionInfo":{"status":"ok","timestamp":1745660534792,"user_tz":-330,"elapsed":62758,"user":{"displayName":"4MH22CA009","userId":"08774515799825101415"}},"outputId":"cddbcc71-8a5d-4496-d527-540e32d34e04"},"execution_count":9,"outputs":[{"name":"stdout","output_type":"stream","text":["enter the prompt:what is capital of india?\n"]},{"output_type":"execute_result","data":{"text/plain":["'The capital of India is **New Delhi**.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["## **Prompts**\n","\n","A `PromptTemplate` works just like a fill-in-the-blank sentence.\n","\n","We define:\n","- The **format** of the sentence\n","- The **name of the placeholder**\n","\n","Then we can insert any value into the placeholder when we want to use the template.\n","\n","For example:\n","```python\n","\"Give me a fun fact about {topic}.\"\n"],"metadata":{"id":"n6UqltA1hGx4"}},{"cell_type":"code","source":["from langchain.prompts import PromptTemplate\n","\n","supe_template = PromptTemplate(\n","    input_variables=[\"character\"],\n","    template=\"Tell me something cool about {character}.\"\n",")\n","\n","supe_prompt = supe_template.format(character=\"Spider-Man\")\n"],"metadata":{"id":"ya0SQnzSfrwR","executionInfo":{"status":"ok","timestamp":1745660534796,"user_tz":-330,"elapsed":6,"user":{"displayName":"4MH22CA009","userId":"08774515799825101415"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["gemini_flash.invoke(supe_prompt).content\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"id":"gzX3-5_Zh4jK","executionInfo":{"status":"ok","timestamp":1745660535608,"user_tz":-330,"elapsed":811,"user":{"displayName":"4MH22CA009","userId":"08774515799825101415"}},"outputId":"20bd0c75-0e1d-4cf5-caf7-35048d41d18b"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"One cool thing about Spider-Man is his incredibly diverse and expansive rogues gallery.  Unlike many superheroes who face a revolving door of generic henchmen, Spider-Man's villains are complex, iconic characters with compelling motivations and backstories, ranging from the tragic (Green Goblin) to the outright bizarre (Mysterio).  This depth allows for endless storytelling possibilities and keeps the character consistently fresh and engaging.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["### defining roles with templates\n","\n","With `ChatPromptTemplate`, we can define roles like:\n","- `\"system\"`: Tells the AI how it should behave\n","- `\"human\"`: The actual message from the user\n","\n","This structure is useful when you're building an AI that needs personality‚Äîlike a superhero trivia expert!\n","\n"],"metadata":{"id":"eHLx5dQkiaTb"}},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","\n","# chat prompt\n","hero_prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"You are a Marvel and DC comics expert. Answer the questions with a bit of flair.\"),\n","    (\"human\", \"Tell me something interesting about {character}.\")\n","])\n"],"metadata":{"id":"blc7G6UpiKHd","executionInfo":{"status":"ok","timestamp":1745660535643,"user_tz":-330,"elapsed":20,"user":{"displayName":"4MH22CA009","userId":"08774515799825101415"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["gemini_flash.invoke(hero_prompt.invoke({\"character\": \"Black Panther\"})).content\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":126},"id":"v_N4I8MTil1h","executionInfo":{"status":"ok","timestamp":1745660537083,"user_tz":-330,"elapsed":1436,"user":{"displayName":"4MH22CA009","userId":"08774515799825101415"}},"outputId":"1a6f786a-7bbd-4e12-a422-33b485ce90b0"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"Ah, the Black Panther!  Beyond his vibranium-enhanced suit and regal bearing, there's a fascinating wrinkle to his power that often gets overlooked:  **the Panther God, Bast.**  It's not just a mystical connection; it's a *symbiotic* one.  The Black Panther isn't *granted* power, he's *fused* with it.  Each Panther who takes the mantle is imbued with enhanced senses, strength, agility, and a heightened connection to the spirit world ‚Äì a direct result of consuming the Heart-Shaped Herb.  This isn't just a temporary boost; it's a fundamental alteration, a mystical merging that elevates the wearer to something beyond mere mortal.  It's a power that's both incredibly potent and deeply spiritually bound, making T'Challa (and his predecessors) far more than just a superhero ‚Äì they're almost a living embodiment of Wakanda's mystical heritage.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["## **Chains**\n","\n","Until now, we‚Äôve:\n","- Defined prompts using `PromptTemplate` or `ChatPromptTemplate`\n","- Passed them manually to Gemini using `.invoke()`\n","\n","That works well for one-off prompts.\n","\n","But what if we want to **tie the prompt and the model together** into a single reusable tool? Something we can use over and over again with different inputs?\n","\n","That‚Äôs exactly what an `LLMChain` does!\n","\n","An `LLMChain` in LangChain is like a mini pipeline:\n","> It takes an input ‚Üí uses your prompt ‚Üí sends it to the model ‚Üí and gives back a response.\n","\n","\n"],"metadata":{"id":"afSUKq1ijQF2"}},{"cell_type":"code","source":["supe_chain = hero_prompt | gemini_flash\n","supe_chain.invoke({\"character\": \"Thor\"}).content\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":108},"id":"E4VrQaAXjiDg","executionInfo":{"status":"ok","timestamp":1745660538341,"user_tz":-330,"elapsed":1257,"user":{"displayName":"4MH22CA009","userId":"08774515799825101415"}},"outputId":"49f52d8c-deb2-4544-f4fd-2d6aa6a8401d"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"Ah, Thor, the God of Thunder!  While his mighty hammer Mjolnir is certainly iconic,  a lesser-known fact that truly showcases his character's depth is his surprisingly profound connection to **Shakespeare**.  Yes, you read that right!  Thor, the Asgardian warrior, has a deep appreciation for the Bard, often quoting Shakespearean lines in battle and even during moments of quiet contemplation. It's a subtle detail, but it adds a layer of unexpected sophistication to this thunderous powerhouse, hinting at a thoughtful, even romantic, side beneath the brawn.  It's a testament to the rich tapestry of his character, proving that even gods can appreciate the beauty of iambic pentameter!\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["! pip install langchain-community wikipedia --quiet"],"metadata":{"id":"na4tGFx7pwcm","executionInfo":{"status":"ok","timestamp":1745660547283,"user_tz":-330,"elapsed":8931,"user":{"displayName":"4MH22CA009","userId":"08774515799825101415"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"612c1861-3ea8-434d-d949-a0457a509070"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"markdown","source":["### Adding an Output Parser to the Chain\n","\n","Sometimes, the output from the model may include unnecessary text, formatting, or structure you don‚Äôt want.\n","\n","With LangChain, you can clean or extract just the part you need by using an **Output Parser**.\n","\n","For example:\n","- Extract just the joke\n","- Return only a fun fact\n","- Parse structured outputs like JSON\n","\n","Let‚Äôs add a basic output parser that just extracts and strips the response text.\n"],"metadata":{"id":"f8eVOUpyrKmE"}},{"cell_type":"code","source":["from langchain_core.output_parsers import StrOutputParser\n","\n","# string output parser\n","output_parser = StrOutputParser()\n"],"metadata":{"id":"zcnF-sJsprqi","executionInfo":{"status":"ok","timestamp":1745660547349,"user_tz":-330,"elapsed":54,"user":{"displayName":"4MH22CA009","userId":"08774515799825101415"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# LCEL chain\n","full_chain = hero_prompt| gemini_flash | output_parser\n"],"metadata":{"id":"InV_19KwrOWj","executionInfo":{"status":"ok","timestamp":1745660547358,"user_tz":-330,"elapsed":9,"user":{"displayName":"4MH22CA009","userId":"08774515799825101415"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["full_chain.invoke({\"character\": \"The Hulk\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145},"id":"uJ5ix4M8rW4c","executionInfo":{"status":"ok","timestamp":1745660548959,"user_tz":-330,"elapsed":1600,"user":{"displayName":"4MH22CA009","userId":"08774515799825101415"}},"outputId":"5f3fcd16-804f-49f2-a1a6-661abbea11e7"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"Ah, the Jade Giant!  There's a mountain of interesting facts about the Hulk, but let's delve into a lesser-known, yet fascinating aspect: **his surprisingly nuanced and evolving intellect.**\\n\\nWhile often portrayed as a raging brute, the Hulk isn't simply a mindless monster.  Depending on his anger level (and there are *many* levels!), his intelligence fluctuates wildly.  In some incarnations, he's displayed astonishing intellect, even surpassing Bruce Banner's in certain forms like Professor Hulk.  This isn't just a matter of gamma-irradiated brainpower; it's a testament to the complex interplay between Banner's scientific mind and the Hulk's primal rage.  The Hulk's intelligence isn't a static thing; it's a reflection of his emotional state, a dynamic and unpredictable element that adds layers of complexity to this iconic character.  He's not just strong; he's capable of surprising strategic thinking and even moments of profound insight, making him far more than just a green rage-monster.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["## **RAG** (Retrieval-Augmented Generation)\n","LLMs are powerful, but they don't know everything‚Äîespecially not your **custom data** like:\n","- PDFs üìÑ\n","- Documents üóÇÔ∏è\n","- Notes üìù\n","- Websites üåê\n","\n","So how do we make an AI that *actually knows* your data?\n","\n","That‚Äôs where **RAG** come in.\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"wxtxZrTGsCfo"}},{"cell_type":"markdown","source":["## üß† What's an RAG?\n","\n","An **RAG** helps the LLM *retrieve relevant information* from a large document or dataset.\n","\n","This concept is often called:\n","- **Indexing**\n","- **Document Retrieval**\n","- **Context Injection**\n"],"metadata":{"id":"9Mz6knLytA16"}},{"cell_type":"markdown","source":["## üóÇÔ∏è How It Works (Step-by-Step)\n","\n","1. **Load** your documents (PDFs, text files, etc.)\n","2. **Split** the docs into smaller chunks\n","3. **Embed** the chunks using an embedding model (convert text ‚Üí numbers)\n","4. **Store** them in a **Vector Store**\n","5. **Retrieve** relevant chunks at query time\n","\n","The LLM uses this real data to answer your questions!\n","\n","> Think of it like giving your AI a *memory boost* from your own notes or documents."],"metadata":{"id":"h1n839KRtDUI"}},{"cell_type":"markdown","source":["### Step 1: Load and Chunk the Document\n","\n","To prepare our fictional superhero database for the LLM, we first load it and break it into smaller chunks.\n","\n","This is important because LLMs have a context window ‚Äî they can't read entire books or long files at once. So we split the data into manageable pieces that can later be searched.\n"],"metadata":{"id":"IGwsoQX-1ciH"}},{"cell_type":"code","source":["from langchain_community.document_loaders import TextLoader\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","\n","# Loading the file\n","loader = TextLoader(\"/content/heros.txt\")\n","docs = loader.load()\n","\n","# Spliting the data into chunks\n","splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n","chunks = splitter.split_documents(docs)\n","\n","print(f\"Number of chunks: {len(chunks)}\")\n","print(f\"Sample chunk:\\n{chunks[0].page_content}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tCST08hcrZ32","executionInfo":{"status":"ok","timestamp":1745661025101,"user_tz":-330,"elapsed":4,"user":{"displayName":"4MH22CA009","userId":"08774515799825101415"}},"outputId":"6a5231b9-c1e8-473f-cc56-d1959e13bc05"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of chunks: 5\n","Sample chunk:\n","Photonix is a superhero who absorbs solar energy and converts it into plasma beams. She works out of Solar City.\n"]}]},{"cell_type":"markdown","source":["### Step 2: Embedding and Vector Store\n","\n","To enable our AI agent to understand and retrieve relevant information, we convert each text chunk into a vector (numerical representation) using **Gemini Embeddings**.\n","\n","We then store these vectors in a **FAISS Vector Store** ‚Äî a lightweight database optimized for fast similarity search.\n","\n","This forms the backbone of our Retrieval-Augmented Generation (RAG) system.\n"],"metadata":{"id":"XxF6pvEX1j1Y"}},{"cell_type":"code","source":["! pip install faiss-cpu --quiet"],"metadata":{"id":"rKbGYARH1u1x","executionInfo":{"status":"ok","timestamp":1745661053947,"user_tz":-330,"elapsed":5681,"user":{"displayName":"4MH22CA009","userId":"08774515799825101415"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"33659aaa-e2af-4903-88a9-73730626fa5c"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["from langchain_community.vectorstores import FAISS\n","from langchain_google_genai import GoogleGenerativeAIEmbeddings\n","\n","# embedding model (Gemini)\n","embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n","\n","# vector store from the document chunks\n","vectorstore = FAISS.from_documents(chunks, embedding_model)\n","\n","vectorstore.save_local(\"faiss_superheroes\")\n"],"metadata":{"id":"OR7YQsw71lpf","executionInfo":{"status":"ok","timestamp":1745661061139,"user_tz":-330,"elapsed":1070,"user":{"displayName":"4MH22CA009","userId":"08774515799825101415"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["### Step 3: Retrieval + Response (RAG)\n","\n","This is the final step of the RAG workflow.\n","\n","We take a user's question, search for the most relevant chunk(s) from our vector store using similarity matching, and send that retrieved context ‚Äî along with the question ‚Äî to Gemini.\n","\n","This way, the LLM can answer using the specific knowledge we provided, even if it wasn‚Äôt originally trained on it.\n"],"metadata":{"id":"qUTYZ52r3L4z"}},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","\n","\n","question = \"make a cool story using all the heroes\"\n","retriever = vectorstore.as_retriever()\n","\n","\n","docs = retriever.get_relevant_documents(question)\n","context = \"\\n\\n\".join(doc.page_content for doc in docs)\n","\n","rag_prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Answer the question using ONLY the provided context.\\n\\nContext:\\n{context}\"),\n","    (\"human\", \"{question}\")\n","])\n","\n","chain = rag_prompt | gemini_flash\n","\n","response = chain.invoke({\"context\": context, \"question\": question})\n","print(response.content)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xk9eMYpP1rwV","executionInfo":{"status":"ok","timestamp":1745661083430,"user_tz":-330,"elapsed":2354,"user":{"displayName":"4MH22CA009","userId":"08774515799825101415"}},"outputId":"58adbeed-541c-456b-b30b-78e0e3208805"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-25-ec8af21bb440>:8: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n","  docs = retriever.get_relevant_documents(question)\n"]},{"output_type":"stream","name":"stdout","text":["In the heart of Solar City, Photonix felt a tremor.  Simultaneously, in the distant Frostville, Cryoblast sensed an unusual surge of heat.  News reports flashed across his ice-cold screen: a massive energy surge originating near Solar City.  He knew Photonix would be involved.\n","\n","Meanwhile, in the shadowy alleys of a nameless city, Shadowstrike, alerted by the Nightwatch League, detected the same energy signature.  His invisibility cloaked him as he moved swiftly towards the source.\n","\n","The source was a rogue energy weapon, capable of unleashing devastating heat waves.  Photonix was battling it, her plasma beams deflecting the intense energy, but the weapon was overwhelming her.  The heat was spreading, threatening to engulf Solar City.\n","\n","Cryoblast arrived, his touch instantly freezing parts of the weapon, slowing its output.  But the weapon was too powerful for him alone.  Then, Brickfist, drawn by the chaos, smashed through a building, his rock-hard fists ready.  He began to dismantle the weapon's outer casing, buying Photonix and Cryoblast precious time.\n","\n","Shadowstrike, using his invisibility, infiltrated the weapon's core, disabling its internal systems.  With the weapon's power significantly reduced, Photonix unleashed a final, concentrated plasma beam, destroying the threat.  The combined efforts of the four heroes had saved Solar City from destruction.  They stood together, a testament to the power of unlikely alliances.\n"]}]},{"cell_type":"markdown","source":["##  Memory: Remembering Previous Conversations\n","\n","AI agents become more useful when they can remember what was said earlier. This is especially important in conversations where the context builds over time.\n","\n","LangChain provides different types of memory. The simplest and most common one is:\n","\n","### Conversation Buffer Memory\n","\n","It remembers everything that has been said so far in the conversation. It's like a chat history stored in memory.\n","\n","We‚Äôll now see how to use `ConversationBufferMemory` with our LLM.\n"],"metadata":{"id":"68BmsmWBIq4R"}},{"cell_type":"code","source":["from langchain.memory import ConversationBufferMemory\n","from langchain.chains import ConversationChain\n","\n","# Define the memory\n","memory = ConversationBufferMemory(return_messages=True)\n","\n","# Create the conversation chain using Gemini\n","conversation = ConversationChain(\n","    llm=gemini_flash,\n","    memory=memory,\n","    verbose=True\n",")\n","\n","\n","response = conversation.predict(input=\"Who is Iron Man?\")\n","print(response)\n","\n","response = conversation.predict(input=\"What is his real name?\")\n","print(response)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SLgKub6QIoFa","executionInfo":{"status":"ok","timestamp":1745661141101,"user_tz":-330,"elapsed":2209,"user":{"displayName":"4MH22CA009","userId":"08774515799825101415"}},"outputId":"2ff0a4c1-2a48-4e64-ebc5-2192427a658c"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-26-501b82361b6c>:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n","  memory = ConversationBufferMemory(return_messages=True)\n","<ipython-input-26-501b82361b6c>:8: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n","  conversation = ConversationChain(\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","[]\n","Human: Who is Iron Man?\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","Iron Man is a superhero appearing in comic books published by Marvel Comics.  His real name is Anthony Edward \"Tony\" Stark.  He's a brilliant, billionaire industrialist, playboy, and philanthropist.  He created a powerful suit of armor to protect himself after being injured and captured by terrorists in a war-torn region (often depicted as being in Afghanistan).  This suit, initially designed as a means of escape, eventually evolved into a sophisticated technological marvel, granting him superhuman strength, flight, and various other abilities.  His suit is constantly being upgraded and improved, incorporating advanced weaponry and defensive systems.  He's known for his quick wit, arrogance, and sometimes reckless behavior, but ultimately possesses a strong sense of justice and a desire to protect the world.  He's a founding member of the Avengers, a team of superheroes dedicated to protecting Earth from global threats.  He's been portrayed in various media, most notably in the Marvel Cinematic Universe (MCU) film series, where he's played by Robert Downey Jr.  The MCU portrayal significantly impacted his popularity and cemented his place in popular culture.  There are numerous comic book storylines, animated series, and video games featuring Iron Man, each offering different interpretations of the character and his adventures.\n","\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","[HumanMessage(content='Who is Iron Man?', additional_kwargs={}, response_metadata={}), AIMessage(content='Iron Man is a superhero appearing in comic books published by Marvel Comics.  His real name is Anthony Edward \"Tony\" Stark.  He\\'s a brilliant, billionaire industrialist, playboy, and philanthropist.  He created a powerful suit of armor to protect himself after being injured and captured by terrorists in a war-torn region (often depicted as being in Afghanistan).  This suit, initially designed as a means of escape, eventually evolved into a sophisticated technological marvel, granting him superhuman strength, flight, and various other abilities.  His suit is constantly being upgraded and improved, incorporating advanced weaponry and defensive systems.  He\\'s known for his quick wit, arrogance, and sometimes reckless behavior, but ultimately possesses a strong sense of justice and a desire to protect the world.  He\\'s a founding member of the Avengers, a team of superheroes dedicated to protecting Earth from global threats.  He\\'s been portrayed in various media, most notably in the Marvel Cinematic Universe (MCU) film series, where he\\'s played by Robert Downey Jr.  The MCU portrayal significantly impacted his popularity and cemented his place in popular culture.  There are numerous comic book storylines, animated series, and video games featuring Iron Man, each offering different interpretations of the character and his adventures.', additional_kwargs={}, response_metadata={})]\n","Human: What is his real name?\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","His real name is Anthony Edward \"Tony\" Stark.\n"]}]},{"cell_type":"markdown","source":["## Agents: Accessing Other Tools\n","\n","Large Language Models (LLMs) are powerful, but they don‚Äôt know everything ‚Äî especially if it's information they weren‚Äôt trained on. That‚Äôs where **agents** come in.\n","\n","### What are Agents?\n","\n","Agents allow an LLM to **decide when and how to use external tools** (like a calculator or Wikipedia search) to answer questions.\n","\n","For example, if you ask an AI:\n","- \"What is 1832 squared?\" ‚Üí It might use a calculator tool.\n","- \"When was Stan Lee born?\" ‚Üí It might use the Wikipedia tool.\n","\n","The model doesn‚Äôt just generate text ‚Äî it reasons out what tools to use and then combines the results to form a response.\n","\n","### Tools We‚Äôre Using\n","\n","- **wikipedia**: To fetch real-time facts from Wikipedia.\n","- **llm-math**: To solve mathematical problems accurately.\n","\n","### How It Works\n","\n","We:\n","1. Load the tools.\n","2. Initialize the agent with those tools and the LLM.\n","3. Ask a question, and the agent decides what tool(s) to use and returns the final answer.\n"],"metadata":{"id":"HL5iGzluJZhn"}},{"cell_type":"markdown","source":["### AI Agent: Wikipedia + Math\n","\n","In this example, we're creating a simple agent that can:\n","- **Retrieve information** from Wikipedia.\n","- **Solve math problems** using `llm-math`.\n","\n","We used the following steps:\n","1. **Loaded tools**: The tools include the **Wikipedia** tool to fetch knowledge and the **llm-math** tool to solve math problems.\n","2. **Initialized the agent**: The agent is configured to use these tools with the `ZERO_SHOT_REACT_DESCRIPTION` agent type, allowing it to handle different types of requests and respond accordingly.\n","3. **Asked a question**: The agent is asked \"When was Stan Lee born?\" and will use Wikipedia to retrieve the relevant answer.\n","\n","Example Task: \"When was Stan Lee born?\"\n","This task combines the agent's ability to search the web and provide an informative answer.\n"],"metadata":{"id":"upOTRH4MXvFM"}},{"cell_type":"code","source":["from langchain.agents import load_tools\n","from langchain.agents import initialize_agent\n","from langchain.agents import AgentType\n","\n","tools = load_tools([\"wikipedia\", \"llm-math\"], llm=gemini_flash)\n","agent = initialize_agent(tools,\n","                         gemini_flash,\n","                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n","                         verbose=True)\n","\n","agent.run(\"when was stan lee born?\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":546},"id":"bkTC6xhjI4Rn","executionInfo":{"status":"ok","timestamp":1745661391002,"user_tz":-330,"elapsed":6125,"user":{"displayName":"4MH22CA009","userId":"08774515799825101415"}},"outputId":"0e52ac51-34b9-40d4-af7e-4b9119b4aa9c"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-27-65c5bf7b9aa2>:6: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n","  agent = initialize_agent(tools,\n","<ipython-input-27-65c5bf7b9aa2>:11: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n","  agent.run(\"when was stan lee born?\")\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3mThought: I need to find Stan Lee's birthdate.  Wikipedia is a good resource for this.\n","Action: wikipedia\n","Action Input: Stan Lee\u001b[0m"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n","\n","The code that caused this warning is on line 389 of the file /usr/local/lib/python3.11/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n","\n","  lis = BeautifulSoup(html).find_all('li')\n"]},{"output_type":"stream","name":"stdout","text":["\n","Observation: \u001b[36;1m\u001b[1;3mPage: Stan Lee\n","Summary: Stan Lee (born Stanley Martin Lieber ; December 28, 1922 ‚Äì November 12, 2018) was an American comic book writer, editor, publisher, and producer. He rose through the ranks of a family-run business called Timely Comics which later became Marvel Comics. He was Marvel's primary creative leader for two decades, expanding it from a small publishing house division to a multimedia corporation that dominated the comics and film industries.\n","In collaboration with others at Marvel ‚Äì particularly co-writers and artists Jack Kirby and Steve Ditko ‚Äì he co-created iconic characters, including Spider-Man, the X-Men, Iron Man, Thor, the Hulk, Ant-Man, the Wasp, the Fantastic Four, Black Panther, Daredevil, Doctor Strange, the Scarlet Witch, and Black Widow. These and other characters' introductions in the 1960s pioneered a more naturalistic approach in superhero comics. In the 1970s, Lee challenged the restrictions of the Comics Code Authority, indirectly leading to changes in its policies. In the 1980s, he pursued the development of Marvel properties in other media, with mixed results.\n","\n","Following his retirement from Marvel in the 1990s, Lee remained a public figurehead for the company. He frequently made cameo appearances in films and television shows based on Marvel properties, on which he received an executive producer credit, which allowed him to become the person with the highest-grossing film total ever.  He continued independent creative ventures until his death, aged 95, in 2018. Lee was inducted into the comic book industry's Will Eisner Award Hall of Fame in 1994 and the Jack Kirby Hall of Fame in 1995. He received the NEA's National Medal of Arts in 2008.\n","\n","Page: List of cameo appearances by Stan Lee\n","Summary: The following is a list of cameo appearances by Stan Lee (1922‚Äì2018) in Marvel Comics films, television shows and video games. A prolific comic book writer, editor, publisher, and producer, Lee has also had numerous cameos in non-Marvel media.\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3mThought: I now know the answer from the Wikipedia observation.\n","Final Answer: Stan Lee was born on December 28, 1922.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["'Stan Lee was born on December 28, 1922.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","source":["### üõ†Ô∏è Common Tools You Can Use in LangChain Agents\n","\n","LangChain comes with several tools that allow your agent to interact with the outside world. Here are some beginner-friendly and useful ones:\n","\n","- **wikipedia** ‚Äì Search and retrieve info from Wikipedia.\n","- **llm-math** ‚Äì Solve basic math problems using an LLM-backed calculator.\n","- **python_repl** ‚Äì Run Python code in a secure environment.\n","- **requests** ‚Äì Make HTTP requests to external APIs (GET, POST).\n","- **serpapi** ‚Äì Perform real-time web search (needs API key).\n","- **google-search** ‚Äì Use Google Search for live results (needs setup).\n","- **terminal** ‚Äì Run shell commands (only in trusted environments).\n","- **file** ‚Äì Read/write files in the local system.\n","- **arxiv** ‚Äì Search for academic papers on arXiv.\n","- **twilio** ‚Äì Send SMS (requires setup and Twilio account).\n","- **open-meteo-api** ‚Äì Get weather updates using the Open-Meteo API.\n","\n","> üìå Note: Some tools may require API keys or additional setup (like `serpapi`, `twilio`, or `google-search`).\n","\n","These tools make your AI agent feel more like a **real assistant** that can search, compute, read, write, and interact with the real world.\n","\n"],"metadata":{"id":"nAyT9YFtMXki"}},{"cell_type":"markdown","source":["### Agent: Web Search + Math\n","\n","In this example, we're creating a fun agent that:\n","- **Searches the web** for real-time information using the **DuckDuckGo Search** tool (`ddg-search`).\n","- **Solves math problems** using the `llm-math` tool.\n","\n","The steps:\n","1. **Loaded tools**: We used the **DuckDuckGo Search** tool to fetch the latest web data and the **llm-math** tool to handle math-based queries.\n","2. **Initialized the agent**: The agent is configured to utilize these tools with the `ZERO_SHOT_REACT_DESCRIPTION` agent type, allowing it to react to various requests without prior knowledge of the specific task.\n","3. **Asked a question**: The agent is tasked with searching for the latest AI news: \"What are the latest AI news?\"\n","\n","Example Task: \"What are the latest AI news?\"\n","This task demonstrates how the agent can pull fresh web data and provide up-to-date information.\n"],"metadata":{"id":"glL61kHRYNov"}},{"cell_type":"code","source":["!pip install google-search-results --quiet"],"metadata":{"id":"SfqtZdd6PFdH","executionInfo":{"status":"aborted","timestamp":1745660549282,"user_tz":-330,"elapsed":114383,"user":{"displayName":"4MH22CA009","userId":"08774515799825101415"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -U duckduckgo-search --quiet"],"metadata":{"id":"rnAgUG5pRb8I","executionInfo":{"status":"ok","timestamp":1745661471284,"user_tz":-330,"elapsed":4137,"user":{"displayName":"4MH22CA009","userId":"08774515799825101415"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9e7a8cf7-0ff1-41c9-a20c-83e5600f4f63"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/3.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.6/3.3 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["tools = load_tools([\"ddg-search\", \"llm-math\"], llm=gemini_flash)\n","agent = initialize_agent(\n","    tools,\n","    llm=gemini_flash,\n","    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n","    verbose=False\n",")\n","\n","agent.run(\"What are the latest AI news?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"id":"z4M6SSi1Jy6n","executionInfo":{"status":"ok","timestamp":1745661475012,"user_tz":-330,"elapsed":3726,"user":{"displayName":"4MH22CA009","userId":"08774515799825101415"}},"outputId":"22527bf4-8eaa-4487-ec47-26a9b5ecbdde"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"This week's top AI news includes significant investment and scaling in companies like OpenAI and Meta, breakthroughs in AI hardware and model efficiency from Google, expansion of open-source AI and global initiatives (particularly in China), and the release of new AI models like Nvidia's LVNM 1.0 and ByteDance's Doubao 1.5 Pro.  There's also been focus on the advancements of LLMs like Google's Gemini and their applications in enterprise solutions.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["### Research Agent: ArXiv Search\n","\n","In this example, we created an agent that:\n","- **Searches ArXiv** for recent research papers using the **ArXiv** tool.\n","- **Answers questions** related to academic research on AI topics.\n","\n","The steps:\n","1. **Loaded the ArXiv tool**: We utilized the **ArXiv** tool to search for the latest papers in the academic field.\n","2. **Initialized the agent**: The agent is configured to use the **ArXiv** tool with the `ZERO_SHOT_REACT_DESCRIPTION` agent type, allowing it to react to queries without needing a specific training task.\n","3. **Asked a research-focused question**: The agent is asked to \"Find recent ArXiv papers on using ensemble methods and explainable AI for brain tumor detection from MRI images.\"\n","\n","Example Task: \"Find recent ArXiv papers on using ensemble methods and explainable AI for brain tumor detection from MRI images.\"\n","This task showcases how the agent can search for specific academic research and provide relevant results.\n","\n","### How it works:\n","- The agent uses **ArXiv** to search for the most recent research papers.\n","- It can then provide insights or summaries of those papers based on the search results.\n","\n","This is a powerful tool for accessing the latest research and generating high-quality academic content.\n"],"metadata":{"id":"Wkzl3VOUYspk"}},{"cell_type":"code","source":["!pip install arxiv pymupdf --quiet"],"metadata":{"id":"FnNaeVbSTggT","executionInfo":{"status":"ok","timestamp":1745661672452,"user_tz":-330,"elapsed":7957,"user":{"displayName":"4MH22CA009","userId":"08774515799825101415"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e7fc6310-2d53-426a-c889-1c0a8804d80c"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["tools = load_tools([\"arxiv\"], llm=gemini_flash)\n","\n","agent = initialize_agent(\n","    tools=tools,\n","    llm = gemini_flash,\n","    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n","    verbose=True\n",")\n","\n","agent.run(\"Find recent ArXiv papers on using ensemble methods and explainable AI for brain tumor detection from MRI images.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ITxf-EwrSH_0","executionInfo":{"status":"ok","timestamp":1745661685021,"user_tz":-330,"elapsed":12568,"user":{"displayName":"4MH22CA009","userId":"08774515799825101415"}},"outputId":"ba535efd-0c1d-418e-821a-a44e6c9170a8"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3mThought: I need to search arXiv for papers combining ensemble methods, explainable AI (XAI), and brain tumor detection from MRI images.  I'll use keywords to refine my search.\n","\n","Action: arxiv\n","Action Input: \"ensemble methods explainable AI brain tumor MRI\"\u001b[0m\n","Observation: \u001b[36;1m\u001b[1;3mPublished: 2024-07-15\n","Title: Brain Tumor Classification From MRI Images Using Machine Learning\n","Authors: Vidhyapriya Ranganathan, Celshiya Udaiyar, Jaisree Jayanth, Meghaa P V, Srija B, Uthra S\n","Summary: Brain tumor is a life-threatening problem and hampers the normal functioning\n","of the human body. The average five-year relative survival rate for malignant\n","brain tumors is 35.6 percent. For proper diagnosis and efficient treatment\n","planning, it is necessary to detect the brain tumor in early stages. Due to\n","advancement in medical imaging technology, the brain images are taken in\n","different modalities. The ability to extract relevant characteristics from\n","magnetic resonance imaging (MRI) scans is a crucial step for brain tumor\n","classifiers. Several studies have proposed various strategies to extract\n","relevant features from different modalities of MRI to predict the growth of\n","abnormal tumors. Most techniques used conventional methods of image processing\n","for feature extraction and machine learning for classification. More recently,\n","the use of deep learning algorithms in medical imaging has resulted in\n","significant improvements in the classification and diagnosis of brain tumors.\n","Since tumors are located at different regions of the brain, localizing the\n","tumor and classifying it to a particular category is a challenging task. The\n","objective of this project is to develop a predictive system for brain tumor\n","detection using machine learning(ensembling).\n","\n","Published: 2024-05-06\n","Title: An Optimized Ensemble Deep Learning Model For Brain Tumor Classification\n","Authors: Md. Alamin Talukder, Md. Manowarul Islam, Md Ashraf Uddin\n","Summary: Brain tumors present a grave risk to human life, demanding precise and timely\n","diagnosis for effective treatment. Inaccurate identification of brain tumors\n","can significantly diminish life expectancy, underscoring the critical need for\n","precise diagnostic methods. Manual identification of brain tumors within vast\n","Magnetic Resonance Imaging (MRI) image datasets is arduous and time-consuming.\n","Thus, the development of a reliable deep learning (DL) model is essential to\n","enhance diagnostic accuracy and ultimately save lives. This study introduces an\n","innovative optimization-based deep ensemble approach employing transfer\n","learning (TL) to efficiently classify brain tumors. Our methodology includes\n","meticulous preprocessing, reconstruction of TL architectures, fine-tuning, and\n","ensemble DL models utilizing weighted optimization techniques such as Genetic\n","Algorithm-based Weight Optimization (GAWO) and Grid Search-based Weight\n","Optimization (GSWO). Experimentation is conducted on the Figshare\n","Contrast-Enhanced MRI (CE-MRI) brain tumor dataset, comprising 3064 images. Our\n","approach achieves notable accuracy scores, with Xception, ResNet50V2,\n","ResNet152V2, InceptionResNetV2, GAWO, and GSWO attaining 99.42%, 98.37%,\n","98.22%, 98.26%, 99.71%, and 99.76% accuracy, respectively. Notably, GSWO\n","demonstrates superior accuracy, averaging 99.76\\% accuracy across five folds on\n","the Figshare CE-MRI brain tumor dataset. The comparative analysis highlights\n","the significant performance enhancement of our proposed model over existing\n","counterparts. In conclusion, our optimized deep ensemble model exhibits\n","exceptional accuracy in swiftly classifying brain tumors. Furthermore, it has\n","the potential to assist neurologists and clinicians in making accurate and\n","immediate diagnostic decisions.\n","\n","Published: 2025-03-31\n","Title: Detecting Glioma, Meningioma, and Pituitary Tumors, and Normal Brain Tissues based on Yolov11 and Yolov8 Deep Learning Models\n","Authors: Ahmed M. Taha, Salah A. Aly, Mohamed F. Darwish\n","Summary: Accurate and quick diagnosis of normal brain tissue Glioma, Meningioma, and\n","Pituitary Tumors is crucial for optimal treatment planning and improved medical\n","results. Magnetic Resonance Imaging (MRI) is widely used as a non-invasive\n","diagnostic tool for detecting brain abnormalities, including tumors. However,\n","manual interpretation of MRI scans is often time-cons\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3mThought: I now know the final answer.  The observation shows several recent arXiv papers related to brain tumor detection using ensemble methods and deep learning. While explicit mention of \"explainable AI\" is limited, the papers focus on improving accuracy and efficiency of brain tumor classification using ensemble techniques and deep learning models.\n","\n","Final Answer: Several recent arXiv papers explore ensemble methods for brain tumor detection from MRI images.  While not all explicitly mention \"explainable AI,\" they focus on improving the accuracy and efficiency of classification using ensemble deep learning models.  Specific examples include papers on optimized ensemble deep learning models achieving high accuracy and a study using YOLOv11 and YOLOv8 for tumor detection.  More information on the specific methods used for explainability would require a deeper dive into the individual papers.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["'Several recent arXiv papers explore ensemble methods for brain tumor detection from MRI images.  While not all explicitly mention \"explainable AI,\" they focus on improving the accuracy and efficiency of classification using ensemble deep learning models.  Specific examples include papers on optimized ensemble deep learning models achieving high accuracy and a study using YOLOv11 and YOLOv8 for tumor detection.  More information on the specific methods used for explainability would require a deeper dive into the individual papers.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","source":["# made by **Bhuvan S.**\n","- 6th semester | dept. CSE(AI) | MITM\n","- CodeCatalyst | Technology"],"metadata":{"id":"BXwmdnSHcs1l"}},{"cell_type":"code","source":[],"metadata":{"id":"PuORfsQnS51k","executionInfo":{"status":"aborted","timestamp":1745660549428,"user_tz":-330,"elapsed":114520,"user":{"displayName":"4MH22CA009","userId":"08774515799825101415"}}},"execution_count":null,"outputs":[]}]}